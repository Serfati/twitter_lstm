{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DK5yIF4H7t5m"
   },
   "outputs": [],
   "source": [
    "PATH = 'assets/twitter1.6m.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4a3f57d5652e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_gpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'test'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "TtAV_bTC7t5q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.5 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: numpy==1.19.4 in /home/serfata/.local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.19.4)\n",
      "Requirement already satisfied: matplotlib==3.3.3 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (3.3.3)\n",
      "Requirement already satisfied: seaborn==0.11.1 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.11.1)\n",
      "Requirement already satisfied: wordcloud==1.8.1 in /home/serfata/.local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.8.1)\n",
      "Requirement already satisfied: plotly==4.6.0 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (4.6.0)\n",
      "Requirement already satisfied: gensim==3.8.3 in /home/serfata/.local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (3.8.3)\n",
      "Requirement already satisfied: scikit_learn==0.24.0 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (0.24.0)\n",
      "Requirement already satisfied: tensorflow-gpu in /home/serfata/.local/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (2.4.0)\n",
      "Requirement already satisfied: pandas==1.1.4 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (1.1.4)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/serfata/.local/lib/python3.7/site-packages (from gensim==3.8.3->-r requirements.txt (line 7)) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/serfata/.local/lib/python3.7/site-packages (from gensim==3.8.3->-r requirements.txt (line 7)) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/serfata/.local/lib/python3.7/site-packages (from gensim==3.8.3->-r requirements.txt (line 7)) (4.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (8.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: regex in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from nltk==3.5->-r requirements.txt (line 1)) (2020.11.13)\n",
      "Requirement already satisfied: joblib in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from nltk==3.5->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from nltk==3.5->-r requirements.txt (line 1)) (4.56.2)\n",
      "Requirement already satisfied: click in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from nltk==3.5->-r requirements.txt (line 1)) (7.1.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from pandas==1.1.4->-r requirements.txt (line 11)) (2020.5)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from plotly==4.6.0->-r requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/serfata/.conda/envs/tweet/lib/python3.7/site-packages (from scikit_learn==0.24.0->-r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (3.7.4.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (0.11.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (0.36.2)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (1.1.2)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (1.32.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (2.4.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (0.3.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (2.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (3.14.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (1.12)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (3.3.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/serfata/.local/lib/python3.7/site-packages (from tensorflow-gpu->-r requirements.txt (line 9)) (1.12.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/serfata/.local/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/serfata/.local/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (0.4.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/serfata/.local/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/serfata/.local/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (3.3.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/serfata/.local/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (2.25.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/serfata/.local/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (1.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/serfata/.local/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (51.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/serfata/.local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/serfata/.local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/serfata/.local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (4.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/serfata/.local/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/serfata/.local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (3.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/serfata/.local/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/serfata/.local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/serfata/.local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/serfata/.local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/serfata/.local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/serfata/.local/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/serfata/.local/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu->-r requirements.txt (line 9)) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "87-_quCtzm_4"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HkaQIWnE7t5q"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jrzevoJP7t5r"
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2bMPCC3y7t5r"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pJn9-Spd7t5s"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import homogeneity_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOnFe6ra7t5s"
   },
   "source": [
    "## **2. Data exploratory analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x48DnLDb7t5t"
   },
   "source": [
    "### **2.1 Data overview**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho9YteIj7t5t"
   },
   "source": [
    "\n",
    "**Dataset**\n",
    "The dataset being used is the given competition twitter  dataset. It contains over 1M tweets. The tweets have been annotated (0 = Negative, 4 = Positive) and they can be used to detect sentiment.\n",
    "We will use 100K tweets: 50K tweets with negative sentiment  (annotated as “negative”) and 50K tweets with positive sentiment (annotated as “positive”).\n",
    "The dataset contains 6 different fields, we will focus the following:\n",
    "    Target (Sentiment): the emotion of the tweet (0 = negative, 4 = positive)\n",
    "    Text: the text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>q</th>\n",
       "      <th>username</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment          id                          date         q  \\\n",
       "0          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "\n",
       "        username                                      SentimentText  \n",
       "0  scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "1       mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "2        ElleCTF    my whole body feels itchy and like its on fire   \n",
       "3         Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "4       joy_wolf                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "raw_df = pd.read_csv(PATH,encoding=DATASET_ENCODING)\n",
    "raw_df.columns = ['Sentiment', 'id', 'date', 'q','username' ,'SentimentText']\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twyDqVf97t5t"
   },
   "source": [
    "### **2.2 Data Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7R2_42p7t5u"
   },
   "source": [
    "The first step is to load the data to global environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TigiU-nyzn4K"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1319382</th>\n",
       "      <td>4</td>\n",
       "      <td>I finally got sometime for myself today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100923</th>\n",
       "      <td>4</td>\n",
       "      <td>@Nicxo yeaaah, well hopefully they will phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226390</th>\n",
       "      <td>4</td>\n",
       "      <td>@punkofevil ooh maybe you'll have fun with gli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454628</th>\n",
       "      <td>4</td>\n",
       "      <td>@wyndwitch don't believe you, not if you are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016644</th>\n",
       "      <td>4</td>\n",
       "      <td>@Tink10270 but u wouldn't trade me for nuthin.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                      SentimentText\n",
       "1319382          4          I finally got sometime for myself today. \n",
       "1100923          4  @Nicxo yeaaah, well hopefully they will phone ...\n",
       "1226390          4  @punkofevil ooh maybe you'll have fun with gli...\n",
       "1454628          4  @wyndwitch don't believe you, not if you are t...\n",
       "1016644          4    @Tink10270 but u wouldn't trade me for nuthin. "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = raw_df.loc[raw_df.Sentiment == 4].sample(n=int(5e4))\n",
    "neg_df = raw_df.loc[raw_df.Sentiment == 0].sample(n=int(5e4))\n",
    "\n",
    "df = pd.concat([pos_df, neg_df], axis=0)[['Sentiment','SentimentText']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib4RRNgL7t5u"
   },
   "source": [
    "We could see some abnormal words such as <br /><br />, then we should replace them by a null or space value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2mypTdx7t5x"
   },
   "source": [
    "### **2.3 Data pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoJs-9B_0fa1"
   },
   "source": [
    "**Text Cleaning**\n",
    "\n",
    "0.Label Encoder\n",
    "\n",
    "1.Remove html tags\n",
    "\n",
    "2.Remove special characters\n",
    "\n",
    "3.Converting every thing to lower case\n",
    "\n",
    "4.Removing Stop words\n",
    "\n",
    "5.Stemming\n",
    "\n",
    "6.Remove extra spaces\n",
    "\n",
    "7.Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m0c9Mhd27t5x",
    "outputId": "b62b9b54-736c-4a30-c245-463c36db9863"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1319382</th>\n",
       "      <td>1</td>\n",
       "      <td>I finally got sometime for myself today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100923</th>\n",
       "      <td>1</td>\n",
       "      <td>@Nicxo yeaaah, well hopefully they will phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226390</th>\n",
       "      <td>1</td>\n",
       "      <td>@punkofevil ooh maybe you'll have fun with gli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454628</th>\n",
       "      <td>1</td>\n",
       "      <td>@wyndwitch don't believe you, not if you are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016644</th>\n",
       "      <td>1</td>\n",
       "      <td>@Tink10270 but u wouldn't trade me for nuthin.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                      SentimentText\n",
       "1319382          1          I finally got sometime for myself today. \n",
       "1100923          1  @Nicxo yeaaah, well hopefully they will phone ...\n",
       "1226390          1  @punkofevil ooh maybe you'll have fun with gli...\n",
       "1454628          1  @wyndwitch don't believe you, not if you are t...\n",
       "1016644          1    @Tink10270 but u wouldn't trade me for nuthin. "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['Sentiment'] = label_encoder.fit_transform(df['Sentiment'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ATF_Itt07t5y",
    "outputId": "a75bafaa-9bd2-4f2f-845d-7e978ea68bd1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/serfata/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading contractions.csv and storing it as a dict.\n",
    "con_path = 'assets/contractions.csv'\n",
    "contractions = pd.read_csv(con_path,index_col='Contraction')\n",
    "contractions.index = contractions.index.str.lower()\n",
    "contractions.Meaning = contractions.Meaning.str.lower()\n",
    "contractions_dict = contractions.to_dict()['Meaning']\n",
    "\n",
    "# Defining regex patterns.\n",
    "urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n",
    "userPattern       = '@[^\\s]+'\n",
    "hashtagPattern    = '#[^\\s]+'\n",
    "alphaPattern      = \"[^a-z0-9<>]\"\n",
    "sequencePattern   = r\"(.)\\1\\1+\"\n",
    "seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "# Defining regex for emojis\n",
    "smileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\n",
    "sademoji          = r\"[8:=;]['`\\-]?\\(+\"\n",
    "neutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n",
    "lolemoji          = r\"[8:=;]['`\\-]?p+\"\n",
    "\n",
    "def preprocess_apply(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Replace all URls with '<url>'\n",
    "    tweet = re.sub(urlPattern,'<url>',tweet)\n",
    "    \n",
    "    # Replace @USERNAME to '<user>'.\n",
    "    tweet = re.sub(userPattern,'<user>', tweet)\n",
    "    \n",
    "    # Replace 3 or more consecutive letters by 2 letter.\n",
    "    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "    \n",
    "    # Replace all emojis.\n",
    "    tweet = re.sub(r'<3', '<heart>', tweet)\n",
    "    tweet = re.sub(smileemoji, '<smile>', tweet)\n",
    "    tweet = re.sub(sademoji, '<sadface>', tweet)\n",
    "    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n",
    "    tweet = re.sub(lolemoji, '<lolface>', tweet)\n",
    "    for contraction, replacement in contractions_dict.items():\n",
    "        tweet = tweet.replace(contraction, replacement)\n",
    "    \n",
    "    # Remove non-alphanumeric and symbols\n",
    "    tweet = re.sub(alphaPattern, ' ', tweet)\n",
    "    \n",
    "    # Adding space on either side of '/' to seperate words (After replacing URLS).\n",
    "    tweet = re.sub(r'/', ' / ', tweet)\n",
    "    return tweet\n",
    "\n",
    "df['SentimentText'] = df.SentimentText.apply(preprocess_apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZhD8qFa7t5z"
   },
   "source": [
    "**Stop Words Removal**\n",
    "\n",
    "We'll remove the stop words for better prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OM2dasP47t5z",
    "outputId": "ac95abe5-5c73-478e-f585-892b30102183"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/serfata/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 100000/100000 [00:10<00:00, 9341.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "nltk.download('punkt')\n",
    "pbar = tqdm(total=df.shape[0], leave=True, position=0)\n",
    "\n",
    "def remove_sw(SentimentText):\n",
    "    tokens = word_tokenize(SentimentText)\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    pbar.update(1)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"SentimentText\"] = df[\"SentimentText\"].apply(remove_sw)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ebb3PAO47t5z"
   },
   "source": [
    "**Porter Stemmer**\n",
    "\n",
    "For this particular dataset the PorterStemmer does not bring better performance, so it is better to skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FQS-rhtY0qEp",
    "outputId": "d117ee1c-cb65-4396-bb55-65589837fa97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:12<00:00, 8267.86it/s]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=df.shape[0], leave=True, position=0)\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    pbar.update(1)\n",
    "    return ' '.join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "df[\"SentimentText\"] = df[\"SentimentText\"].apply(stem)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WXblOqOj7t50",
    "outputId": "aa4091c3-6643-4a14-c297-293d72efdc0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/serfata/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "#Creating a Lemmatizer for preprocessing\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8LzxOHtJ7t50"
   },
   "outputs": [],
   "source": [
    "df[\"SentimentText\"] = df[\"SentimentText\"].apply(lambda x: re.sub(\" +\",\" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Reviews polarity')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sentiment reviews distribution\n",
    "s = df['Sentiment'].value_counts()\n",
    "s = (s/s.sum())*100\n",
    "\n",
    "plt.figure()\n",
    "bars = plt.bar(s.index, s.values, color = ['green', 'red'], alpha = .6)\n",
    "plt.xticks(s.index, ['Positive', 'Negative'], fontsize = 15)\n",
    "plt.tick_params(bottom = False, top = False, left = False, right = False, labelleft = False)\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 5, s = str(bar.get_height())[:2] + '%', ha = 'center', fontsize = 15)\n",
    "plt.title('Reviews polarity', fontsize = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dXDprN1i6OlJ"
   },
   "outputs": [],
   "source": [
    "X = df['SentimentText']\n",
    "y = df['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoKXLtZi7t54"
   },
   "source": [
    "Split data to train and test for modeling and performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kIeTkIFa1Byt",
    "outputId": "6f7d8a71-6857-4e1e-897d-5348e29f2132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset : 90000 SentimentTexts\n",
      "Testing dataset : 10000 SentimentTexts\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "print('Training dataset : {} SentimentTexts'.format(X_train.shape[0]))\n",
    "print('Testing dataset : {} SentimentTexts'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kxGk8KXE7t54",
    "outputId": "275ba806-54b0-4856-f3ef-7db3729a44db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5061\n",
       "0    4939\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT4YKKBjAuhm"
   },
   "source": [
    "## **3. Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I53-aoEu7t55"
   },
   "source": [
    "### 3.1 Feature Extraction using TF-IDF algorithm\n",
    "\n",
    "![TFIDF](https://miro.medium.com/max/532/0*bHkPdhgfnyTs4un_)\n",
    "\n",
    "In scikit-learn, the TF-IDF algorithm is implemented using **TfidfTransformer**. This transformer needs the count matrix which it will transform later. Hence, we use **CountVectorizer** first.\n",
    "Alternatively, one can use **TfidfVectorizer**, which is the equivalent of CountVectorizer followed by TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "03QUaef_1BsZ",
    "outputId": "d144758e-f122-48a5-efd6-56533b2bb9e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/serfata/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "tfidfVect =    TfidfVectorizer( max_df = 0.5,\n",
    "                                sublinear_tf=True,\n",
    "                                lowercase = True, \n",
    "                                ngram_range = (1,2), \n",
    "                                tokenizer = LemmaTokenizer(),\n",
    "                                stop_words = 'english',\n",
    "                                min_df = 1,\n",
    "                                use_idf = True,\n",
    "                                # max_features = 1000,\n",
    "                                strip_accents = 'ascii'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xy5Kji8F7t55",
    "outputId": "b6407d1a-3adb-42b2-fabe-6fc1f61f72e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 s, sys: 177 ms, total: 14.4 s\n",
      "Wall time: 14.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(90000, 376440)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time features = tfidfVect.fit_transform(X_train)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qsa2RRxP2JPg"
   },
   "outputs": [],
   "source": [
    "features_test = tfidfVect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRMY4_Zi7t59"
   },
   "source": [
    "## Prediction Models: Logistic Regression and DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfIdfVectorizer Feature Extraction\n",
    "\n",
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.7622666666666668\n",
      "Best params:  {'clf__tol': 0.0001, 'vect__max_df': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#instantiate the model (with the default parameters)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "#fit the model with data (occurs in-place)\n",
    "lr.fit(features, y_train)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#tune parameters - hyper parameter optimizer\n",
    "lr_clf = Pipeline([('vect', TfidfVectorizer()),('clf', LogisticRegression())])\n",
    "parameters =  {'vect__max_df': (0.1,0.5),'clf__tol': (0.0001,0.001)}\n",
    "gs_clf_b = GridSearchCV(lr_clf, parameters, n_jobs=1, cv=KFold(n_splits=10, shuffle=True, random_state=0))\n",
    "gs_clf_b = gs_clf_b.fit(X_train,y_train)\n",
    "print('Best score: ',gs_clf_b.best_score_)\n",
    "print('Best params: ',gs_clf_b.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**DNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and Padding datasets\n",
    "\n",
    "**Tokenization** is a common task in **Natural Language Processing (NLP)**. It’s a fundamental step in both traditional NLP methods like **Count Vectorizer** and Advanced Deep Learning-based architectures like **Transformers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab length: 39205\n"
     ]
    }
   ],
   "source": [
    "# Defining the model input length.\n",
    "input_length = 60\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_length = len(tokenizer.word_index) + 1\n",
    "print(\"Tokenizer vocab length:\", vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data.shape: (90000, 60)\n"
     ]
    }
   ],
   "source": [
    "X_data = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\n",
    "print(\"X_data.shape:\", X_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Word Embeddings using Word2Vec model\n",
    "\n",
    "**Word embedding** is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. Loosely speaking, word embeddings are **vector representations** of a particular word.\n",
    "\n",
    "**Word2Vec** was developed by Google and is one of the most popular technique to learn word embeddings using shallow neural network.\n",
    "Word2Vec can create word embeddings using two methods (both involving Neural Networks): **Skip Gram** and **Common Bag Of Words (CBOW)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding_dimensions = 100\n",
    "embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/home/serfata/.conda/envs/tweet/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape: (39205, 100)\n",
      "CPU times: user 583 ms, sys: 121 ms, total: 704 ms\n",
      "Wall time: 909 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "cbow = KeyedVectors.load('assets/cbow')\n",
    "\n",
    "for word, token in tokenizer.word_index.items():\n",
    "    if cbow.wv.__contains__(word):\n",
    "        embedding_matrix[token] = cbow.wv.__getitem__(word)\n",
    "\n",
    "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "There are different approches which we can use to build our Sentiment analysis model. We're going to build a deeplearning **Sequence model.**\n",
    "\n",
    "**Sequence model** are very good at getting the context of a sentence, since it can understand the meaning rather than employ techniques like counting positive or negative words like in a **Bag-of-Words model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    embedding_layer = Embedding(input_dim = vocab_length,\n",
    "                                output_dim = Embedding_dimensions,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=input_length,\n",
    "                                trainable=False)\n",
    "    model = Sequential([\n",
    "        embedding_layer,\n",
    "        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n",
    "        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n",
    "        Conv1D(100, 5, activation='relu'),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 60, 100)           3920500   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 60, 200)           160800    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 60, 200)           240800    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 56, 100)           100100    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 4,423,833\n",
      "Trainable params: 503,333\n",
      "Non-trainable params: 3,920,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "training_model = getModel()\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Callbacks\n",
    "\n",
    "**Callbacks** are objects that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau,TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_board = TensorBoard(\n",
    "    log_dir='logs', histogram_freq=0, write_graph=True,\n",
    "    write_images=True, update_freq='epoch', profile_batch=2,\n",
    "    embeddings_freq=0, embeddings_metadata=None\n",
    ")\n",
    "\n",
    "callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "             EarlyStopping(monitor='val_acc', min_delta=1e-6, patience=15),\n",
    "             tensor_board\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Metrices\n",
    "\n",
    "**Metric:** We've selected **Accuracy** as it is one of the common evaluation metrics in classification problems when the category data is equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.metrics as metrics\n",
    "\n",
    "mymetrics=['acc',metrics.Precision(), metrics.Recall(), metrics.AUC(), metrics.RootMeanSquaredError()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compile\n",
    "\n",
    "The Model must be compiled to define the **loss, metrics and optimizer**. Defining the proper loss and metric is essential while training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=mymetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now train our model using the **fit** method and store the output learning parameters in **history**, which can be used to plot out the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14/21 [===================>..........] - ETA: 1:05 - loss: 0.6455 - acc: 0.6176 - precision: 0.6238 - recall: 0.5725 - auc: 0.6652 - root_mean_squared_error: 0.4767"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9f271c8ce7e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/.conda/envs/tweet/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tweet/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tweet/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tweet/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tweet/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tweet/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/tweet/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = training_model.fit(\n",
    "    X_data,\n",
    "    y_train,\n",
    "    batch_size=4096,\n",
    "    epochs=50,\n",
    "    validation_split=0.05,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're saving the **tokenizer and Tensorflow model** for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Saving the tokenizer\n",
    "with open('Tokenizer.pickle', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "# Saving the TF-Model.\n",
    "training_model.save('Twitter-Sentiment-BiLSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeUJKV2h7t6F"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = lr.predict(features_test)\n",
    "pred = pd.DataFrame(pred)\n",
    "print(accuracy_score(y_test, pred))\n",
    "print(recall_score(y_test, pred))\n",
    "print(precision_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.heatmap(confusion_matrix(y_test, pred), annot=True, cmap=\"icefire\",xticklabels=['Negative', 'Positive'],yticklabels=\n",
    "['Negative', 'Positive'], fmt='g')\n",
    "\n",
    "sn.color_palette(\"pastel\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('LogisticRegression Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10FOLD CROSS VALIDATION approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "def show_metrics(title,accuracy,precision,recall):\n",
    "    # joined graph\n",
    "    pyplot.title(title)\n",
    "    pyplot.plot(accuracy, label='accuracy')\n",
    "    pyplot.legend()\n",
    "    pyplot.plot(precision, label='precision')\n",
    "    pyplot.legend()\n",
    "    pyplot.plot(recall, label='recall')\n",
    "    pyplot.legend(loc=\"center left\")\n",
    "    pyplot.show()\n",
    "    \n",
    "    # seperated graphs\n",
    "    pyplot.title(f\"Accuracy\")\n",
    "    pyplot.plot(accuracy, label='accuracy')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    pyplot.title(f\"Precision\")\n",
    "    pyplot.plot(precision, label='precision')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    pyplot.title(f\"Recall:\")\n",
    "    pyplot.plot(recall, label='recall')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "lr_tfidf_fold_models = []\n",
    "lr_tfidf_fold_accuracy = []\n",
    "lr_tfidf_fold_precision = []\n",
    "lr_tfidf_fold_recall = []\n",
    "max_accuracy = 0\n",
    "lr_tfidf_best_confusion_matrix = []\n",
    "\n",
    "k_fold = KFold(n_splits=10, shuffle=False)\n",
    "counter = 1\n",
    "tfidf_features = tfidfVect.fit_transform(X)\n",
    "for train_index, validation_index in k_fold.split(tfidf_features):\n",
    "\n",
    "    logistic_regression = LogisticRegression(random_state=0, max_iter = 1000)\n",
    "    X_train = tfidf_features[train_index]\n",
    "    Y_train = y[train_index]\n",
    "    logistic_regression.fit(X_train,Y_train)\n",
    "\n",
    "    X_valid = tfidf_features[validation_index]\n",
    "    Y_valid = y[validation_index]\n",
    "    Y_valid = Y_valid.values\n",
    "    predictions = logistic_regression.predict(X_valid)\n",
    "\n",
    "    lr_tfidf_fold_models.append(logistic_regression)\n",
    "    lr_tfidf_fold_accuracy.append(accuracy_score(Y_valid, predictions))\n",
    "    lr_tfidf_fold_precision.append(precision_score(Y_valid, predictions))\n",
    "    lr_tfidf_fold_recall.append(recall_score(Y_valid, predictions))\n",
    "\n",
    "    print(f'--------------- Fold {counter} ------------------')\n",
    "    print(f\"Accuracy: {lr_tfidf_fold_accuracy[counter-1]} \")\n",
    "    print(f\"Precision: {lr_tfidf_fold_precision[counter-1]} \")\n",
    "    print(f\"Recall: {lr_tfidf_fold_recall[counter-1]} \")\n",
    "    if max_accuracy < lr_tfidf_fold_accuracy[counter-1]:\n",
    "        lr_tfidf_best_confusion_matrix = confusion_matrix(Y_valid,predictions)\n",
    "        max_accuracy = lr_tfidf_fold_accuracy[counter-1]\n",
    "    counter+=1\n",
    "\n",
    "\n",
    "print('-----------------------------------------')\n",
    "show_metrics('Logistic Regression - TFIDFvectorization',lr_tfidf_fold_accuracy,lr_tfidf_fold_precision,lr_tfidf_fold_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEATMAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.heatmap(lr_tfidf_best_confusion_matrix, annot=True, cmap=\"icefire\",xticklabels=['Negative', 'Positive'],yticklabels=\n",
    "['Negative', 'Positive'], fmt='g')\n",
    "sn.color_palette(\"pastel\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Logistic Regression TFIDF')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet",
   "language": "python",
   "name": "tweet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
